{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from transformers.models.vit.modeling_vit import ViTSelfAttention\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('./vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('./vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSelfAttention(ViTSelfAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states, head_mask, output_attentions: bool = False):\n",
    "        print('this is the sparse attention')\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # filter the attention scores by threshold only save the top-k values\n",
    "        # save top-30% of the values and set the rest to -inf\n",
    "\n",
    "        k = int(attention_scores.size(-1) * 0.3)\n",
    "        k = max(k, 1)  # 确保至少保留一个元素\n",
    "\n",
    "        # 在最后一个维度上获取前k个最大的注意力得分及其索引\n",
    "        topk_values, topk_indices = torch.topk(attention_scores, k, dim=-1)\n",
    "\n",
    "        # 创建一个与attention_scores形状相同的全为负无穷的张量\n",
    "        mask = torch.full_like(attention_scores, float('-inf'))\n",
    "\n",
    "        # 将前k个注意力得分填充回对应的位置\n",
    "        mask.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "        # 更新注意力得分\n",
    "        attention_scores = mask\n",
    "\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, ViTSelfAttention):\n",
    "        module.__class__ = SparseSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "this is the sparse attention\n",
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jctorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
