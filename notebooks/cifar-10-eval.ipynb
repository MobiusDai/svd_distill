{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifar-10 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"/data/jc/datasets/cifar-10\", split=\"train\", streaming=True)\n",
    "val_dataset = load_dataset(\"/data/jc/datasets/cifar-10\", split=\"test\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('../weights/vit-base-patch16-224-in21k-finetuned-cifar10/')\n",
    "\n",
    "def preprocess_function(item):\n",
    "    # Resize the input image to the model's size\n",
    "    inputs = processor(images=item[\"img\"], return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = item[\"label\"]\n",
    "    return inputs \n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, remove_columns=[\"img\"], batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, remove_columns=[\"img\"], batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "def collect_fn(batch):\n",
    "    batch = {    \n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch], dim=0),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])    \n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compulate_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = np.mean(preds == labels)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('../weights/vit-base-patch16-224-in21k-finetuned-cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "#     if any(nd in name for nd in ['norm', 'head', 'patch_embed', 'downsample']):\n",
    "#         continue\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 测试完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2564162611961365,\n",
       " 'eval_accuracy': 0.9788,\n",
       " 'eval_runtime': 30.7967,\n",
       " 'eval_samples_per_second': 324.711,\n",
       " 'eval_steps_per_second': 0.227}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collect_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    compute_metrics=compulate_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=400,\n",
    "        output_dir=\"./logs\",\n",
    "        max_steps=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 测试低秩模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying low rank on vit.encoder.layer.0.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.0.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.0.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.0.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.0.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.0.output.dense\n",
      "applying low rank on vit.encoder.layer.1.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.1.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.1.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.1.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.1.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.1.output.dense\n",
      "applying low rank on vit.encoder.layer.2.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.2.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.2.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.2.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.2.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.2.output.dense\n",
      "applying low rank on vit.encoder.layer.3.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.3.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.3.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.3.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.3.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.3.output.dense\n",
      "applying low rank on vit.encoder.layer.4.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.4.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.4.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.4.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.4.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.4.output.dense\n",
      "applying low rank on vit.encoder.layer.5.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.5.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.5.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.5.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.5.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.5.output.dense\n",
      "applying low rank on vit.encoder.layer.6.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.6.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.6.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.6.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.6.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.6.output.dense\n",
      "applying low rank on vit.encoder.layer.7.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.7.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.7.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.7.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.7.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.7.output.dense\n",
      "applying low rank on vit.encoder.layer.8.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.8.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.8.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.8.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.8.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.8.output.dense\n",
      "applying low rank on vit.encoder.layer.9.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.9.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.9.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.9.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.9.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.9.output.dense\n",
      "applying low rank on vit.encoder.layer.10.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.10.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.10.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.10.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.10.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.10.output.dense\n",
      "applying low rank on vit.encoder.layer.11.attention.attention.query\n",
      "applying low rank on vit.encoder.layer.11.attention.attention.key\n",
      "applying low rank on vit.encoder.layer.11.attention.attention.value\n",
      "applying low rank on vit.encoder.layer.11.attention.output.dense\n",
      "applying low rank on vit.encoder.layer.11.intermediate.dense\n",
      "applying low rank on vit.encoder.layer.11.output.dense\n",
      "applying low rank on classifier\n",
      "Original model params: 85806346, Low rank model params: 43332606\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import low_rank\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('../weights/vit-base-patch16-224-in21k-finetuned-cifar10')\n",
    "\n",
    "count_params = sum(p.numel() for p in model.parameters())\n",
    "model_lr_transform = low_rank.ModuleLowRank(compress_ratio=2, \n",
    "                                name_omit=['norm', 'head', 'patch_embed', 'downsample'],\n",
    "                                is_approximate=True)\n",
    "low_rank_model = model_lr_transform(model)\n",
    "count_lr_params = sum(p.numel() for p in low_rank_model.parameters())\n",
    "\n",
    "print(f'Original model params: {count_params}, Low rank model params: {count_lr_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular value params: 16588\n"
     ]
    }
   ],
   "source": [
    "sv_params = 0\n",
    "for name, param in low_rank_model.named_parameters():\n",
    "    if 'sv' in name:\n",
    "        sv_params += param.numel()\n",
    "\n",
    "print(f'Singular value params: {sv_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/ubuntu/anaconda3/envs/jcenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0316827297210693,\n",
       " 'eval_accuracy': 0.6297,\n",
       " 'eval_runtime': 30.5201,\n",
       " 'eval_samples_per_second': 327.653,\n",
       " 'eval_steps_per_second': 0.229}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=low_rank_model,\n",
    "    data_collator=collect_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    compute_metrics=compulate_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=400,\n",
    "        output_dir=\"./logs\",\n",
    "        max_steps=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
