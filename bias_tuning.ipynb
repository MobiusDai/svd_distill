{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Êï∞ÊçÆÈõÜÂä†ËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from bias_tuning.datasets import build_transform \n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    input_size: int = 224\n",
    "    color_jitter: float = 0.3\n",
    "    aa: str = 'rand-m9-mstd0.5-inc1'\n",
    "    train_interpolation: str = 'bicubic'\n",
    "    reprob: float = 0.25\n",
    "    remode: str = 'pixel'\n",
    "    recount: int = 1\n",
    "    eval_crop_ratio: float = 0.875\n",
    "\n",
    "data_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('./vit-base-patch16-224')\n",
    "\n",
    "def process_example(example):\n",
    "    if example['image'].mode != 'RGB':\n",
    "        example['image'] = example['image'].convert('RGB')\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['label']\n",
    "    return inputs\n",
    "\n",
    "trainset = load_dataset('/data/jc/dataset/imagenet-1k', split='train', streaming=True)\n",
    "valset = load_dataset('/data/jc/dataset/imagenet-1k', split='validation', streaming=True)\n",
    "\n",
    "prepared_trainset = trainset.map(process_example)\n",
    "prepared_valset = valset.map(process_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê®°ÂûãÊê≠Âª∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import low_rank\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('./vit-base-patch16-224')\n",
    "\n",
    "count_params = sum(p.numel() for p in model.parameters())\n",
    "model_lr = low_rank.ModuleLowRank(compress_ratio=2, \n",
    "                                name_omit=['norm', 'head', 'patch_embed', 'downsample'],\n",
    "                                is_approximate=True)\n",
    "model = model_lr(model)\n",
    "count_lr_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Original model params: {count_params}, Low rank model params: {count_lr_params}')\n",
    "\n",
    "# only set bias to be optimized\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "count_learn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Learnable params: {count_learn_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def collect_fn(batch):\n",
    "    batch = {\n",
    "        'pixel_values': torch.cat([x['pixel_values'] for x in batch], dim=0),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compulate_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jctorch/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='18750000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      11/18750000 05:43 < 199029:02:53, 0.03 it/s, Epoch 0.00/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.613436</td>\n",
       "      <td>0.521820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    # train epochs and batch size\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=128,  # batch size per device during training\n",
    "    per_device_eval_batch_size=128,   # batch size for evaluation\n",
    "    max_steps= int(50*1.2e7/128),\n",
    "\n",
    "    # learning rate and warmup steps\n",
    "    learning_rate=5e-4,               # initial learning rate\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    \n",
    "    # logging\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    report_to='tensorboard',\n",
    "\n",
    "    # evaluation\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=prepared_trainset,         # training dataset\n",
    "    eval_dataset=prepared_valset,             # evaluation dataset\n",
    "    tokenizer=processor,\n",
    "    data_collator=collect_fn,\n",
    "    compute_metrics=compulate_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jctorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
